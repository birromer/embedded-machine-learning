#+TITLE: Embedded Machine Learning

* Introduction
This project aims at implementing and testing machine learning algorithms adapted for an embedded environment. The following classifiers were used:

- Classification and Regression Trees (CART)
- Support Vector Machines (SVM)
- Artificial Neural Networks (ANN)

The task at hand is the identification of musical styles, which is tackled using the [[https://www.kaggle.com/carlthome/gtzan-genre-collection][GTZAN Genre Collection]] dataset. Here, each audio file is stored using the [[https://en.wikipedia.org/wiki/Au_file_format][AU file format]]. A reader has been implemented for this file format, as well as a feature extraction using a [[https://en.wikipedia.org/wiki/Short-time_Fourier_transform#Discrete-time_STFT][Discrete Short-time Fourier transform]].

A full description of the project can be found in the [[https://gitlab.ensta-bretagne.fr/reynetol/embedded-machine-learning][original repository]], together with the base helper routines provided.

* Setup
The project is organized as follows:

#+begin_src bash :exports results :results output
tree -n -L 2 -I 'build|CMake*|__pycache__'
#+end_src

#+RESULTS:
#+begin_example
.
├── ANN
│   ├── prediction_ann.cpp
│   └── train_ann.py
├── CART
│   ├── CART.py
│   ├── Crop recommandations
│   ├── Crop recommandations.pdf
│   ├── prediction_cart.cpp
│   └── train_cart.py
├── DATA
│   ├── ann_weights.txt
│   ├── blues
│   ├── classical
│   ├── country
│   ├── disco
│   ├── features.csv
│   ├── features_prof.csv
│   ├── features_testing.csv
│   ├── features_training.csv
│   ├── file_list_test.txt
│   ├── file_list_train.txt
│   ├── hiphop
│   ├── jazz
│   ├── metal
│   ├── pop
│   ├── reggae
│   ├── rock
│   ├── svm_coeff.csv
│   ├── svm_feat_stats.csv
│   └── tests.org
├── eval.sh
├── Evaluation
│   ├── evaluate_ann.cpp
│   ├── evaluate_cart.cpp
│   └── evaluate_svm.cpp
├── Extraction
│   ├── features_extraction.cpp
│   ├── features_extraction.h
│   └── main.cpp
├── Helpers
│   ├── au_reading.h
│   ├── etypes.h
│   ├── file_helpers.h
│   ├── globals.h
│   ├── music_style_helpers.cpp
│   ├── music_style_helpers.h
│   ├── print_helpers.h
│   ├── signal.h
│   └── wav_reading.h
├── readme.org
├── RF
├── setup.sh
└── SVM
    ├── prediction_svm.cpp
    └── train_svm.py

18 directories, 37 files
#+end_example

Each of the classifiers has its folder (=ANN/=, =CART/=, =RF/=, =SVM/=) with a training script, usually in Python, for it does not have to be embedded, and a classifier in C++.

- =DATA/= :: Contains the audio files from the dataset. They were not included because of their size, but this view shows the necessary organization for reproducible results.

- =Extraction/= :: Contains the feature extraction code, implemented only using STFT.

- =Helpers/= :: Contains a multitude of utilities, such as for signal processing, global definitions, and labels management.

- =Evaluation/= :: Contains programs that measure the accuracy, execution time and memory usage of the algorithms. They are invoked individually for each classifier.

=[[./setup.sh][setup.sh]]= and =[[./eval.sh][eval.sh]]= are the tangled snippets presented in this file, with everything up to model training in the former, and the performance evaluation in the latter.

** Requirements
:PROPERTIES:
:CUSTOM_ID: sec.requirements
:END:

All of the libraries used should be included in a standard building system installation. The only addition is [[https://www.tensorflow.org/][Tensorflow]], for training neural networks. Installation instructions can be found in the [[https://www.tensorflow.org/install/pip][official documentation]].

** Usage
The first step is to compile the source code, which can be done with the following block, working for both a personal computer and a Raspberry Pi board.

#+begin_src bash :tangle "setup.sh" :exports code :results silent :mkdirp yes
# build feature extraction
mkdir -p build/setup
cmake -S Extraction -B build/setup  # "Extraction" may be replaced by the desired module
make -C build/setup                 # or . for the entire project
#+end_src

Only the needed module is compiled now, as some files needed for further building the modules are generated in the following steps. An example is CART, which will only have its prediction tree after the training done in Python.

*** Feature extraction
The extraction of features from the audio files must happen before the training steps. In our case we'll be using only the dataset, which is split into training and testing data.

It outputs to the data folder the files =[[./DATA/file_list_train.txt][file_list_train.txt]]= and =[[./DATA/file_list_test.txt][file_list_test.txt]]=, with the file paths for training and testing. The testing one is the input for the evaluation binaries, if no other is provided.

The features are extracted to the files =[[./DATA/features_training.csv][features_training.csv]]= and =[[./DATA/features_testing.csv][features_testing.csv]]=. They correspond to the aforementioned data and are already computed to avoid repeated calls.

Having the project built, the extraction can be executed with the following command:

#+begin_src bash :tangle "setup.sh" :exports code :results silent
# extract features
./build/setup/EXTRACTION
#+end_src

*** CART
In order to use the CART algorithm, you must first build the classification tree:

#+begin_src bash :tangle "setup.sh" :results silent
# train cart tree
python3 CART/train_cart.py
#+end_src

This will generate the file =[[./CART/prediction_cart.cpp][CART/prediction_cart.cpp]]=, with a function corresponding to a sequence of if/else's analog to the trained binary tree paths.

*** SVM
So to use the SVM model, we must first also execute the related Python script:

#+begin_src bash :tangle "setup.sh" :export code :results silent
# train svm model
python3 SVM/train_svm.py
#+end_src

This will generate the files =[[./DATA/svm_coeff.csv][DATA/svm_coeff.csv]]=, with the weights and bias for the hyperplanes in the model, and =[[./DATA/svm_feat_stats.csv][DATA/svm_feat_stats.csv]]=, with the statistical attributes of the features used in training, so that it can be replicated during prediction.

*** ANN
The usage of the ANN method requires first the training of the associated neural network, also specified in a Python script:

#+begin_src bash :tangle "setup.sh" :export code
# train ann
python3 ANN/train_ann.py
#+end_src

<COMPLETE WITH OUTPUT WEIGHTS FILE>

*** Evaluation
In order to run the evaluation of the learning methods, it is necessary first to compile the Evaluation module, as in the following block:

#+begin_src bash :tangle "eval.sh" :export code :results silent
# build evaluation code
mkdir -p build/eval
cmake -S Evaluation -B build/eval -DUSE_TESTS_FILE=OFF  # ON for using testing paths file
make -C build/eval
#+end_src

The features used for testing have already been computed in the Extraction module, and the necessary learned attributes from their training scripts. As said before, everything is stored in the =Data/= folder.

If desired, it is possible to use the compilation variable *USE_TESTS_FILE* so to ignore the extracted features and extract them from a list of paths in =[[./DATA/file_list_train.txt][DATA/file_list_train.txt]]=. This file is also redundantly computed during extraction, matching the separation of training and testing datasets, so if no modification in done they should both provide the same results, it will just take longer if on. To use it:
#+begin_src bash :tangle no :export code :results silent
cmake -S Evaluation -B build/eval -DUSE_TESTS_FILE=ON
#+end_src

An executable is generated per method, with execution exemplified bellow:
#+begin_src bash :tangle "eval.sh" :export code :results silent
# CART evaluation
./build/eval/EVALUATION_CART

# SVM evaluation
./build/eval/EVALUATION_SVM

# ANN evaluation
./build/eval/EVALUATION_ANN
#+end_src

In practice, the codes related to the prediction using each algorithm is stored in their respective folder, and they are used for this statistical performance analysis defined withing the Evaluation module.

* Development

** CART

*** Space complexity

*** Time complexity

** SVM
The SVM algorithm was first tested on python using the [[https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html][=linearSVC=]] function from /sklearn/, with accuracy values that did not match the ones informed by the professor with his feature set. With that in mind, the pipeline was changed so to use the [[https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC][=SVC=]] function with a linear kernel, consistently providing accuracy values over 10% higher (around 64%, better shown in #sec.results).

The interpretation of the coefficients for predicting the classes from new data in C++ had to change as well, as they represent the plans dividing the results in 1 vs 1 duels withing all classes, in a total of 45. This is better explained in the [[https://scikit-learn.org/stable/modules/svm.html#multi-class-classification][multi-class classification section]] of its documentation.

The training script is located at =[[./SVM/train_svm.py][SVM/train_svm.py]]= and the prediction at =[[./SVM/prediction_svm.cpp][SVM/prediction_svm.cpp]]=.

*** Space complexity

*** Time complexity

** ANN
The training part of the implementation was done in Python using the /Tensorflow/ library, as mentioned in #sec.requirements. Here, a 3 layered neural network was built using 2 dense layers with ReLU activation and 1 output layer with softmax activation, as the results may be interpreted a probability distribution and their sum should total 1.

The [[https://keras.io/api/optimizers/rmsprop/][RMSprop]] optimizer was added to the training of the model, balancing the step sizes according to the magnitude of the gradient during back-propagation. A normal [[https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy][sparse categorical cross entropy]] loss function was used, working with the type of data we have. The model was training with a validation split of 60/40.

The training script is located at [[file:ANN/train_ann.py][=train_ann.py=]]

*** Space complexity

*** Time complexity

* Results and Analysis
:PROPERTIES:
:CUSTOM_ID: sec.results
:END:
