#+TITLE: Embedded Machine Learning

* Introduction
This project aims at implementing and testing machine learning algorithms adapted for an embedded environment. The following classifiers were used:

- Classification and Regression Trees (CART)
- Support Vector Machines (SVM)
- Artificial Neural Networks (ANN)

The task at hand is the identification of musical styles, which is tackled using the [[https://www.kaggle.com/carlthome/gtzan-genre-collection][GTZAN Genre Collection]] dataset. Here, each audio file is stored using the [[https://en.wikipedia.org/wiki/Au_file_format][AU file format]]. A reader has been implemented for this file format, as well as a feature extraction using a [[https://en.wikipedia.org/wiki/Short-time_Fourier_transform#Discrete-time_STFT][Discrete Short-time Fourier transform]].

A full description of the project can be found in the [[https://gitlab.ensta-bretagne.fr/reynetol/embedded-machine-learning][original repository]], together with the base helper routines provided.

* Setup
The project is organized as follows:

#+begin_src bash :exports results :results output
tree -n -L 2 -I 'build|CMake*|__pycache__'
#+end_src

#+RESULTS:
#+begin_example
.
├── ANN
│   ├── prediction_ann.cpp
│   └── train_ann.py
├── CART
│   ├── CART.py
│   ├── Crop recommandations
│   ├── Crop recommandations.pdf
│   ├── prediction_cart.cpp
│   └── train_cart.py
├── DATA
│   ├── ann_weights.txt
│   ├── blues
│   ├── classical
│   ├── country
│   ├── disco
│   ├── features.csv
│   ├── features_prof.csv
│   ├── features_testing.csv
│   ├── features_training.csv
│   ├── file_list_test.txt
│   ├── file_list_train.txt
│   ├── hiphop
│   ├── jazz
│   ├── metal
│   ├── pop
│   ├── reggae
│   ├── rock
│   ├── svm_coeff.csv
│   ├── svm_feat_stats.csv
│   └── tests.org
├── eval.sh
├── Evaluation
│   ├── evaluate_ann.cpp
│   ├── evaluate_cart.cpp
│   └── evaluate_svm.cpp
├── Extraction
│   ├── features_extraction.cpp
│   ├── features_extraction.h
│   └── main.cpp
├── Helpers
│   ├── au_reading.h
│   ├── etypes.h
│   ├── file_helpers.h
│   ├── globals.h
│   ├── music_style_helpers.cpp
│   ├── music_style_helpers.h
│   ├── print_helpers.h
│   ├── signal.h
│   └── wav_reading.h
├── readme.org
├── RF
├── setup.sh
└── SVM
    ├── prediction_svm.cpp
    └── train_svm.py

18 directories, 37 files
#+end_example

Each of the classifiers has its folder (=ANN/=, =CART/=, =RF/=, =SVM/=) with a training script, usually in Python, for it does not have to be embedded, and a classifier in C++.

- =DATA/= :: Contains the audio files from the dataset. They were not included because of their size, but this view shows the necessary organization for reproducible results.

- =Extraction/= :: Contains the feature extraction code, implemented only using STFT.

- =Helpers/= :: Contains a multitude of utilities, such as for signal processing, global definitions, and labels management.

- =Evaluation/= :: Contains programs that measure the accuracy, execution time and memory usage of the algorithms. They are invoked individually for each classifier.

[[./setup.sh][setup.sh]] and [[./eval.sh][eval.sh]] are the tangled snippets presented in this file, with everything up to model training in the former, and the performance evaluation in the latter.

** Requirements
:PROPERTIES:
:CUSTOM_ID: sec.requirements
:END:

All of the libraries used should be included in a standard building system installation. The only addition is [[https://www.tensorflow.org/][Tensorflow]], for training neural networks. Installation instructions can be found in the [[https://www.tensorflow.org/install/pip][official documentation]].

** Usage
The first step is to compile the source code, which can be done with the following block, working for both a personal computer and a Raspberry Pi board.

#+begin_src bash :tangle "setup.sh" :exports code :results silent :mkdirp yes
# build feature extraction
mkdir -p build/setup
cmake -S Extraction -B build/setup  # "Extraction" may be replaced by the desired module
make -C build/setup                 # or . for the entire project
#+end_src

Only the needed module is compiled now, as some files needed for further building the modules are generated in the following steps. An example is CART, which will only have its prediction tree after the training done in Python.

*** Feature extraction
The extraction of features from the audio files must happen before the training steps. In our case we'll be using only the dataset, which is split into training and testing data.

It outputs to the data folder the files [[./DATA/file_list_train.txt][file_list_train.txt]] and [[./DATA/file_list_test.txt][file_list_test.txt]], with the file paths for training and testing. The testing one is the input for the evaluation binaries, if no other is provided.

The features are extracted to the files [[./DATA/features_training.csv][features_training.csv]] and [[./DATA/features_testing.csv][features_testing.csv]]. They correspond to the aforementioned data and are already computed to avoid repeated calls.

Having the project built, the extraction can be executed with the following command:

#+begin_src bash :tangle "setup.sh" :exports code :results silent
# extract features
./build/setup/EXTRACTION
#+end_src

*** CART
In order to use the CART algorithm, you must first build the classification tree:

#+begin_src bash :tangle "setup.sh" :results silent
# train cart tree
python3 CART/train_cart.py
#+end_src

This will generate the file [[./CART/prediction_cart.cpp][CART/prediction_cart.cpp]], with a function corresponding to a sequence of if/else's analog to the trained binary tree paths.

*** SVM
So to use the SVM model, we must first also execute the related Python script:

#+begin_src bash :tangle "setup.sh" :export code :results silent
# train svm model
python3 SVM/train_svm.py
#+end_src

This will generate the files [[./DATA/svm_coeff.csv][DATA/svm_coeff.csv]], with the weights and bias for the hyperplanes in the model, and [[./DATA/svm_feat_stats.csv][DATA/svm_feat_stats.csv]], with the statistical attributes of the features used in training, so that it can be replicated during prediction of new data.

*** ANN
The usage of the ANN method requires first the training of the associated neural network, also specified in a Python script:

#+begin_src bash :tangle "setup.sh" :export code
# train ann
python3 ANN/train_ann.py
#+end_src

This will generate the file [[file:DATA/ann_weights.txt][DATA/ann_weights.txt]], with the weights and bias for all layers of the trained model.

*** Evaluation
In order to run the evaluation of the learning methods, it is necessary first to compile the Evaluation module, as in the following block:

#+begin_src bash :tangle "eval.sh" :export code :results silent
# build evaluation code
mkdir -p build/eval
cmake -S Evaluation -B build/eval -DUSE_TESTS_FILE=OFF  # ON for using testing paths file, described bellow
make -C build/eval
#+end_src

The features used for testing have already been computed in the Extraction module, and the necessary learned attributes from their training scripts. As said before, everything is stored in the =Data/= folder.

If desired, it is possible to use the compilation variable *USE_TESTS_FILE* so to ignore the extracted features and extract them from a list of music files paths in [[./DATA/file_list_train.txt][DATA/file_list_train.txt]]. This file is also redundantly computed during extraction, matching the separation of training and testing datasets, so if no modification in done they should both provide the same results, it will just take longer if left on. To use it:
#+begin_src bash :tangle no :export code :results silent
cmake -S Evaluation -B build/eval -DUSE_TESTS_FILE=ON
#+end_src

An executable is generated per method, an no extra arguments are required as everything extracted from specific files, with execution exemplified bellow:
#+begin_src bash :tangle "eval.sh" :export code :results silent
# CART evaluation
./build/eval/EVALUATION_CART

# SVM evaluation
./build/eval/EVALUATION_SVM

# ANN evaluation
./build/eval/EVALUATION_ANN
#+end_src

Here, the codes related to the prediction using each algorithm is stored in their respective folder, and they are used for the statistical performance analysis defined within the Evaluation module.

* Implementation

** CART
The generation of the CART profits from the [[file:CART/CART.py][implementation]] made available from the original repository, having it only to be adapted for the already split data and some testing was done with varying max depths. Some problems were encountered in its usage from failing expansion of the nodes, with invalid thresholds, but we could not find the source of the problem. Sometimes the training fails with a TypeError, but with repeated executions it works.

The main addition was the C++ code generation from the learned tree, were a recursive depth-first exploration generates conditionals that replicate the paths in code. The generated file is called [[file:SVM/prediction_svm.cpp][SVM/prediction_svm.cpp]], and has a function that retrieves a literal string with the class from a given feature vector.

*** Time complexity
During training, considering /n/ as the number of dimensions in a feature and /m/ as the number data elements in the l

During prediction, considering /d/ as the depth of the learned binary tree, nothing but its transversal is performed, which corresponds to $O(log(d))$ complexity. /d/ is a bounded constant specified before training.

*** Space complexity

** SVM
The SVM algorithm was first tested on python using the [[https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html][=linearSVC=]] function from /sklearn/, with accuracy values that did not match the ones informed by the professor with his feature set. With that in mind, the pipeline was changed so to use the [[https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC][=SVC=]] function with a linear kernel, consistently providing accuracy values over 10% higher (around 64%, better shown in #sec.results).

The interpretation of the coefficients for predicting the classes from new data in C++ had to change as well, as they represent the plans dividing the results in 1 vs 1 duels withing all classes, in a total of 45. This is better explained in the [[https://scikit-learn.org/stable/modules/svm.html#multi-class-classification][multi-class classification section]] of its documentation. In practice, a 45x512 matrix is traversed row-wise, computing the inner product with the feature vector, so to retrieve a value representing the division between the compared classes, where positive values represent a "win" for the first class of the pair, and the second class otherwise.

The training script is located at [[./SVM/train_svm.py][SVM/train_svm.py]] and the prediction at [[./SVM/prediction_svm.cpp][SVM/prediction_svm.cpp]].

*** Time complexity
For training, taking /n/ as the number of dimensions in a feature and /m/ as the number of data elements,

For predicting, now considering /c/ as the number of classes, the result will come from processing Y in $Y = A \times X + B$, where A is a matrix of $(c * (c-1)/2$ rows and /n/ columns with the learned SVM coefficients, and B vector of size /c/ with the bias. Being a single threaded application, $O(c m)$ approximates the execution of its two nested loops, ignoring the "processing" part of Y as it's linear to the size of Y (a.k.a. number of rows in A)

*** Space complexity

** ANN
The training part of the implementation was done in Python using the /Tensorflow/ library, as mentioned in #sec.requirements. Here, a 3 layered neural network was built using 2 dense layers with [[https://keras.io/api/layers/activations/][ReLU activation]] and 1 output layer with [[https://www.tensorflow.org/api_docs/python/tf/nn/softmax][softmax activation]], as the results may be interpreted a probability distribution and their sum should total 1.

The [[https://keras.io/api/optimizers/rmsprop/][RMSprop]] optimizer was added to the training of the model, balancing the step sizes according to the magnitude of the gradient during back-propagation. A normal [[https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy][sparse categorical cross entropy]] loss function was used, working with the type of data we have. The model was training with a validation split of 60/40.

For performing predictions of classes in C++, the feed-forward step was implemented for a network of arbitrary architecture, but limited to ReLU and softmax activation functions. A custom text file is generated from the training script in order to save the weights and biases, with information on how many layers there are, and each layer starting with information of its dimensions. Each layer is represented by a MxN matrix, where each of the M rows represent a neuron composed of N weights, matching the input.

Recent additions to the C++17 and C++20 standards were used to make comprehension easier and more straightforward, those being [[https://en.cppreference.com/w/cpp/algorithm/transform_reduce][transform]], [[https://en.cppreference.com/w/cpp/algorithm/transform_reduce][transform_reduce]] and [[https://en.cppreference.com/w/cpp/algorithm/inner_product][inner_product]] functions, some also present in the SVM implementation.

The training script is located at [[file:ANN/train_ann.py][ANN/train_ann.py]] and the prediction in [[file:ANN/prediction_ann.cpp][ANN/prediction_ann.cpp]].

*** Time complexity
For computing the complexity during training, we'll consider /n/ as the number of dimensions in a feature, /m/ as the number of data elements, /l/ as the number of layers, /n_i/ as the number of neurons in the layer i.

When predicting classes, each feature vector of size /n/ will lead to /l/ matrix multiplications, always between matrices of shape /n_{i-2}/ x /n_{i-1} and /n_{i-1}/ x /n_{i}/, where i in {0..l}. This means that operations of n^3 time complexity will be performed /l/ times, with an added bias addition that can be ignored for notation. Between layers, either a softmax or ReLU activation is applied, but those are linear to their input and will disappear in worst case, leading to $O(l * n^3)$ complexity.

*** Space complexity

* Results and Analysis
:PROPERTIES:
:CUSTOM_ID: sec.results
:END:
